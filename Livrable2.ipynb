{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2uHKuHEw8qt"
      },
      "source": [
        "# **PROJET LEYENDA - LIVRABLE 2 : TRAITEMENT D'IMAGE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XvA17VlzKv_"
      },
      "source": [
        "   ## **INTRODUCTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDUXASg40RG1"
      },
      "source": [
        "Dans le cadre du projet TouNum, l'objectif est de créer une solution entièrement automatisée pour analyser et générer des légendes d'images (captioning).\n",
        "Une étape essentielle pour atteindre cet objectif est d'améliorer la qualité des images afin de les rendre plus adaptées aux tâches ultérieures.\n",
        "\n",
        "La qualité des images peut avoir un impact significatif sur la précision des modèles de classification et de génération de légendes, en particulier lorsqu'on travaille avec des données bruitées ou de faible qualité.\n",
        "Par conséquent, une phase de prétraitement efficace des images est indispensable.\n",
        "\n",
        "Dans ce livrable, le but est de traiter un ensemble d'images bruitées et d'améliorer leur qualité grâce à une technique de débruitage.\n",
        "La méthode choisie repose sur l'utilisation d'autoencodeurs convolutionnels, un type d'architecture de réseau de neurones spécialement conçue pour le débruitage d'images.\n",
        "Les autoencodeurs convolutionnels combinent des couches de convolution, qui sont efficaces pour préserver les relations spatiales dans les images, avec la capacité de l'autoencodeur à apprendre des représentations efficaces des données d'entrée.\n",
        "Cette approche vise à supprimer le bruit tout en conservant les détails importants, ce qui donne des images de meilleure qualité pouvant améliorer la performance des algorithmes en aval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjK1DfV04t9b"
      },
      "source": [
        "## **Théorie : les DAE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJACOvla5BVa"
      },
      "source": [
        "Le Denoising Autoencoder est une extension des autoencodeurs classiques, spécifiquement conçue pour améliorer la qualité des données en éliminant le bruit. Contrairement à un autoencodeur traditionnel qui apprend à reproduire ses entrées, le DAE est entraîné avec des images volontairement dégradées, sur lesquelles du bruit est ajouté. L'objectif est que le modèle apprenne à reconstruire l'image d'origine à partir de cette version altérée.\n",
        "\n",
        "Cette approche présente plusieurs avantages majeurs. En premier lieu, elle empêche le réseau de se contenter de copier mécaniquement l'entrée, ce qui pourrait arriver avec un autoencodeur classique trop complexe. Ensuite, elle permet au modèle de se concentrer sur les caractéristiques importantes de l'image, en apprenant à distinguer les détails pertinents du bruit parasite. Enfin, cette stratégie renforce la robustesse du système face à des données de mauvaise qualité, ce qui est crucial dans le cadre du projet où les images brutes sont souvent bruitées ou de faible résolution.\n",
        "\n",
        "Grâce au Denoising Autoencoder, les images sont nettoyées avant d'être utilisées par les étapes suivantes du pipeline, comme la classification ou la génération de légendes. Cela contribue à améliorer la précision globale du système de traitement d'images automatique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loaMDruJ6MeW"
      },
      "source": [
        "![Architecture d'un DAE](DAE.jpeg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TYTkFQZ5ryW"
      },
      "source": [
        "Pour entraîner efficacement le Denoising Autoencoder, il est essentiel d'évaluer la qualité de la reconstruction de l'image propre à partir de sa version bruitée. Cela se fait à l'aide d'une fonction de perte (loss function), qui mesure l'écart entre l'image originale et l'image reconstruite.\n",
        "\n",
        "Deux fonctions de perte sont couramment utilisées selon le format des données d'entrée :\n",
        "\n",
        "Mean Squared Error (MSE) : utilisée lorsque les pixels des images sont exprimés par des valeurs continues (par exemple entre 0 et 1 ou 0 à 255). Cette fonction calcule la moyenne des carrés des écarts entre les pixels de l'image originale et ceux de l'image générée.\n",
        "\n",
        "Binary Cross-Entropy (BCE) : préférable lorsque les images sont binarisées, c'est-à-dire que les pixels prennent uniquement les valeurs 0 ou 1. Cette fonction mesure la différence entre les distributions de pixels de l'entrée et de la sortie.\n",
        "\n",
        "Le choix de la fonction de perte dépend donc du type de données utilisées, et joue un rôle crucial dans l'optimisation des performances du modèle de débruitage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMLrlfKp_9NY"
      },
      "source": [
        "## 1. Prétraitement et préparation de l'environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6zCz_fU6a3k"
      },
      "outputs": [],
      "source": [
        "# Import des librairies\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "import random\n",
        "import cv2\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array, load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# Affiche les graphiques Matplotlib directement dans le notebook Jupyter.\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le dossier existant c:\\Users\\mallo\\OneDrive\\Bureau\\CESI 2022 - 2025\\Annee 5\\Semestre 10\\Option data science\\Project\\Data-science-project-A5\\Dataset\\Dataset2 a été supprimé.\n",
            "Dataset extrait dans le dossier : c:\\Users\\mallo\\OneDrive\\Bureau\\CESI 2022 - 2025\\Annee 5\\Semestre 10\\Option data science\\Project\\Data-science-project-A5\\Dataset\\Dataset2\n"
          ]
        }
      ],
      "source": [
        "# Extraction du dataset\n",
        "def extract_dataset(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    Extrait les fichiers d'un dataset zippé.\n",
        "\n",
        "    Args:\n",
        "        zip_path (str): Chemin vers le fichier zip.\n",
        "        extract_to (str): Dossier où extraire les fichiers.\n",
        "    \"\"\"\n",
        "    # Convertir les chemins en chemins absolus pour éviter les erreurs\n",
        "    zip_path = os.path.abspath(zip_path)\n",
        "    extract_to = os.path.abspath(extract_to)\n",
        "\n",
        "    if not os.path.exists(zip_path):\n",
        "        raise FileNotFoundError(f\"Le fichier {zip_path} n'existe pas.\")\n",
        "    \n",
        "    # Si le dossier de destination existe, le supprimer\n",
        "    if os.path.exists(extract_to):\n",
        "        shutil.rmtree(extract_to)\n",
        "        print(f\"Le dossier existant {extract_to} a été supprimé.\")\n",
        "\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Aplatir la structure du zip : extraire les fichiers directement dans extract_to\n",
        "        for member in zip_ref.namelist():\n",
        "            filename = os.path.basename(member)\n",
        "            # Vérifie si c'est un fichier zip contenant 'Livrable 1 - Photo' et 'Livrable 2' dans le nom\n",
        "            if filename and filename.endswith('.zip') and any(keyword in filename for keyword in ['Livrable 1 - Photo', 'Livrable 2']):\n",
        "                source = zip_ref.open(member)\n",
        "                target_path = os.path.join(extract_to, filename)\n",
        "                with source, open(target_path, \"wb\") as target:\n",
        "                    target.write(source.read())\n",
        "        \n",
        "\n",
        "    # Extrait le contenu de tous les sous-zip **directement dans extract_to**\n",
        "    for file in os.listdir(extract_to):\n",
        "        if file.endswith('.zip'):\n",
        "            sub_zip_path = os.path.join(extract_to, file)\n",
        "            with ZipFile(sub_zip_path, 'r') as sub_zip_ref:\n",
        "                for member in sub_zip_ref.namelist():\n",
        "                    # Ne garde que le nom du fichier sans les sous-dossiers internes\n",
        "                    filename = os.path.basename(member)\n",
        "                    if filename:\n",
        "                        source = sub_zip_ref.open(member)\n",
        "                        target_file = os.path.join(extract_to, filename)\n",
        "                        with source, open(target_file, \"wb\") as target:\n",
        "                            target.write(source.read())\n",
        "            os.remove(sub_zip_path)  # Supprime le zip après extraction\n",
        "    \n",
        "    print(f\"Dataset extrait dans le dossier : {extract_to}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appel de la fonction pour extraire le dataset\n",
        "dataset_zip_path = './Dataset/Datasets.zip'\n",
        "extract_dataset(dataset_zip_path, './Dataset/Dataset2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "wIuoj-gbKu-r",
        "outputId": "a67a8ccc-7ab5-403b-8f9e-42a7917b4c99"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "dataset_path = '/Dataset/Dataset2/'\n",
        "img_size = (128, 128)  # Taille de l'image\n",
        "num_samples = 6\n",
        "\n",
        "# 1. Chargement et vérification des images\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dossier introuvable : {dataset_path}\")\n",
        "\n",
        "def load_images(folder_path):\n",
        "    \"\"\"Charge toutes les images en mémoire et assure la cohérence des canaux\"\"\"\n",
        "    images = []\n",
        "    valid_ext = ('.png', '.jpg', '.jpeg')\n",
        "\n",
        "\n",
        "    for f in os.listdir(folder_path):\n",
        "        if f.lower().endswith(valid_ext):\n",
        "            img = Image.open(os.path.join(folder_path, f))\n",
        "            # Converti en RGB si necessaire pour assurer la coherence\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            images.append(np.array(img))\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"Aucune image valide trouvée\")\n",
        "\n",
        "    return images\n",
        "\n",
        "images = load_images(dataset_path) # Charger les images d'origine\n",
        "\n",
        "# Chargement des images du deuxième dataset\n",
        "second_dataset_path = '/content/drive/My Drive/Projet/Dataset Livrable 1 - Photo/Photo'\n",
        "Second_images = load_images(second_dataset_path)\n",
        "\n",
        "# 2. Fonction pour obtenir les résolutions des images d'origine\n",
        "def check_image_resolutions(folder):\n",
        "    resolutions = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            resolutions.append(img.shape[:2])  # (hauteur, largeur)\n",
        "    return resolutions\n",
        "\n",
        "resolutions = check_image_resolutions(dataset_path)\n",
        "heights = [res[0] for res in resolutions]\n",
        "widths = [res[1] for res in resolutions]\n",
        "\n",
        "\n",
        "\n",
        "# Affichage des résolutions d'origine\n",
        "#print(\"Distribution des résolutions (hauteur, largeur) des images:\")\n",
        "#print(list(zip(heights, widths)))\n",
        "\n",
        "# 3. Affichage de quelques images avec leurs résolutions d'origine\n",
        "fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "for i in range(min(num_samples, len(images))):  # Sélectionner quelques images au hasard pour les afficher\n",
        "    axes[i].imshow(images[i])\n",
        "    axes[i].set_title(f\" {heights[i]}x{widths[i]}\")\n",
        "    axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovA0wSd3yvMb"
      },
      "source": [
        "## Redimensionnement du Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBcB42vcy6Nu"
      },
      "source": [
        "Le fait que nos images aient différentes résolutions crée des défis pour l'entraînement d'un modèle de machine learning, notamment pour les réseaux de neurones convolutionnels (CNN), qui nécessitent des images uniformes. Voici pourquoi le redimensionnement est important :\n",
        "\n",
        "1. **Uniformisation des données** : En redimensionnant toutes les images à une taille commune (par exemple, 128x128 ou 256x256), nous assurons que chaque image a le même nombre de pixels, facilitant ainsi l'apprentissage.\n",
        "\n",
        "2. **Optimisation des performances** : Les résolutions élevées demandent plus de mémoire et ralentissent l'entraînement. Le redimensionnement permet de réduire la complexité tout en gardant l'essentiel des informations visuelles.\n",
        "\n",
        "3. **Réduction des biais** : Des images de tailles différentes peuvent introduire un biais, où les plus grandes images sont privilégiées. Le redimensionnement garantit un traitement équitable de toutes les images.\n",
        "\n",
        "4. **Simplification du prétraitement** : Le redimensionnement facilite également les étapes de prétraitement (comme l'ajout ou le débruitage) en garantissant des tailles uniformes.\n",
        "\n",
        "Le choix de la taille de redimensionnement dépend du compromis entre la conservation des détails visuels et les exigences en ressources. Par exemple, 128x128 est adapté pour des projets légers, tandis que 256x256 conserve plus de détails mais consomme plus de mémoire et de calcul."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxi64bHQLCYx"
      },
      "outputs": [],
      "source": [
        "# 4. Fonction pour Redimensionner les images\n",
        "def resize_images(images, target_size=img_size):\n",
        "    \"\"\"Redimensionne toutes les images et les convertit en RGB\"\"\"\n",
        "    resized_images = []\n",
        "    for img in images:\n",
        "        img_pil = Image.fromarray(img).convert('RGB')  # Forcer en RGB\n",
        "        img_resized = img_pil.resize(target_size)\n",
        "        resized_images.append(np.array(img_resized))\n",
        "    return np.array(resized_images)\n",
        "\n",
        "\n",
        "\n",
        "resized_images = resize_images(images, target_size=img_size)\n",
        "resized_second_images = resize_images(Second_images, target_size=img_size)\n",
        "# Sauvegarder pour les prochaines utilisations\n",
        "#np.save('/content/drive/My Drive/Projet/second_images.npy', resized_second_images)\n",
        "# Charger directement les images sauvegardées\n",
        "#second_images = np.load('/content/drive/My Drive/Projet/second_images.npy', allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbVDo-VoPqfV"
      },
      "source": [
        "## Affichage d'échantillon de data correcte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "iLYn-j1qLFVi",
        "outputId": "7e6c7b78-81b1-4b93-a1e4-2176af08b516"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5. Affichage des images redimensionnées\n",
        "\n",
        "all_images = np.concatenate([resized_images, resized_second_images], axis=0)\n",
        "all_images = np.array(all_images)\n",
        "print(f\"Nombre d'images dans notre dataset : {len(all_images)}\")\n",
        "\n",
        "\n",
        "def display_images(images, num_samples=5):\n",
        "    \"\"\"Affiche un échantillon d'images\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(min(num_samples, len(all_images))):\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()  # Ajout pour éviter le chevauchement\n",
        "    plt.show()\n",
        "\n",
        "# Appel de la fonction\n",
        "display_images(all_images)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un_FzaptQl8W"
      },
      "source": [
        "Dans cette partie, nous chargeons les images du dataset, vérifions leurs résolutions et les redimensionnons à une taille uniforme (256x256 pixels). Cela garantit que toutes les images ont les mêmes dimensions, ce qui est essentiel pour l’entraînement du modèle d'autoencodeur.\n",
        "\n",
        "Nous pouvons également constaté que nous n'avons que 148 images dans notre Dataset ce qui est très peu pour notre modèle. C'est un risque de surapprentissage (overfitting). Avec un petit dataset, il est facile pour un modèle de surapprendre les données, c'est-à-dire de mémoriser les images spécifiques au lieu d'apprendre des motifs généraux.\n",
        "\n",
        "Un petit dataset limite la quantité d'informations que le modèle peut exploiter, ce qui pourrait limiter ses performances finales, surtout sur des architectures complexes comme les autoencodeurs convolutionnels, qui ont beaucoup de paramètres à entraîner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cjViwXSb1DY"
      },
      "source": [
        "## Analyse des pixels et des couleurs des images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JcmDQ8GY07r",
        "outputId": "609c7fc6-fce1-4af5-b00d-df44393e11e6"
      },
      "outputs": [],
      "source": [
        "#Analyse des pixels : calcul des statistiques\n",
        "\n",
        "def calculate_image_statistics(all_images):\n",
        "    \"\"\"Calcule des statistiques sur les images\"\"\"\n",
        "    pixel_values  = np.concatenate([img.ravel() for img in images]) #Rassemble tous les pixels de toutes les images en un seul tableau unidimensionnel\n",
        "    mean = np.mean(pixel_values)\n",
        "    std = np.std(pixel_values)\n",
        "    median = np.median(pixel_values)\n",
        "\n",
        "    return mean, std, median, pixel_values\n",
        "\n",
        "mean, std, median, pixel_values = calculate_image_statistics(all_images)\n",
        "\n",
        "#Affichage des statistiques\n",
        "print(f\"Moyenne : {mean}\")\n",
        "print(f\"Écart type : {std}\")\n",
        "print(f\"Médiane : {median}\")\n",
        "\n",
        "#Fonction pour visualiser la distribution des couleurs\n",
        "def plot_color_distribution(all_images):\n",
        "    \"\"\"Affiche la distribution des couleurs des images\"\"\"\n",
        "    reds = np.concatenate([img[:, :, 2].ravel() for img in images])\n",
        "    greens = np.concatenate([img[:, :, 1].ravel() for img in images])\n",
        "    blues = np.concatenate([img[:, :, 0].ravel() for img in images])\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(reds, bins=50, color='red', alpha=0.5, label='Rouge') #extrait la composante rouge de chaque image, aplatit ces valeurs en un vecteur puis concatène tous ces vecteurs en un seul grand vecteur reds\n",
        "    plt.hist(greens, bins=50, color='green', alpha=0.5, label='Vert')\n",
        "    plt.hist(blues, bins=50, color='blue', alpha=0.5, label='Bleu')\n",
        "    plt.title(\"Distribution des couleurs des images\")\n",
        "    plt.xlabel(\"Valeur des pixel (0-255)\")\n",
        "    plt.ylabel(\"Nombre de pixels\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRyVXG3kOGF"
      },
      "source": [
        "Dans notre jeu d'images, après avoir analysé les valeurs des pixels, voici les résultats obtenus :\n",
        "\n",
        "Moyenne des pixels : 117.68\n",
        "\n",
        "Écart-type des pixels : 71.70\n",
        "\n",
        "Médiane des pixels : 115.00\n",
        "\n",
        "\n",
        "Interprétation des résultats :\n",
        "Moyenne des pixels : La moyenne est proche du centre de la plage [0, 255], indiquant que nos images ont une luminosité moyenne légèrement plus sombre que le milieu de l'échelle. Elles contiennent un bon équilibre entre zones sombres et claires, mais avec une légère prédominance de zones sombres.\n",
        "\n",
        "Médiane des pixels : La médiane est très proche de la moyenne, ce qui montre que la distribution des pixels est relativement symétrique, sans valeurs extrêmes.\n",
        "\n",
        "Écart-type : L'écart-type élevé (71.70) indique une grande variabilité des pixels, avec un bon contraste entre les zones très sombres et très claires, ce qui est utile pour les algorithmes de traitement d'image.\n",
        "\n",
        "\n",
        "Nos images ont une luminosité légèrement inférieure à la moyenne, une distribution symétrique des valeurs de pixels et un bon contraste. Cela est idéal pour les phases de traitement d'image, car cela permet au modèle de mieux détecter les détails et les transitions dans les images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "CcXUpHKyj6Wk",
        "outputId": "08423cdc-893e-4ba5-c8ec-cb6d276eb389"
      },
      "outputs": [],
      "source": [
        "plot_color_distribution(all_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoAji-BLlBMR"
      },
      "source": [
        "Contraste dans les Images :\n",
        "Les pics aux extrêmes (0 et 1) dans la distribution des pixels indiquent un fort contraste dans les images, ce qui est important pour la netteté des bords et les zones de transition rapide. Lors du traitement, il est crucial de préserver ces zones pour ne pas altérer les détails fins et maintenir la clarté de l'image.\n",
        "\n",
        "Variabilité des Couleurs :\n",
        "Les différentes distributions entre les canaux (rouge, vert, bleu) montrent une bonne diversité des couleurs, bien que les teintes bleutées prédominent légèrement. Cela peut influencer la manière dont l'autoencodeur doit traiter chaque canal pour améliorer la qualité globale tout en respectant cette variation de couleurs.\n",
        "\n",
        "Ajout de Bruit et Dégradation :\n",
        "Lors de l'ajout de bruit, il est essentiel d'observer comment le bruit affecte les pics aux extrêmes et les zones intermédiaires. Lors du débruitage, l'autoencodeur doit être capable de reconstruire correctement ces zones de fort contraste et les couleurs dominantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQqfmR3n6a3o"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSvvuoaL6a3o"
      },
      "outputs": [],
      "source": [
        "# Nouvelle fonction de data augmentation\n",
        "def augment_images(images, augment_factor=2): # Augmente le nombre d'images\n",
        "    \"\"\"Génère des images augmentées\"\"\"\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='reflect' #Les bords sont remplis par effet miroir : les valeurs de pixels sont copiées en sens inversé\n",
        "   )\n",
        "\n",
        "    augmented_images = []\n",
        "    for img in images:\n",
        "        img = img.reshape((1,) + img.shape)\n",
        "        for _ in range(augment_factor):\n",
        "            for batch in datagen.flow(img, batch_size=1):\n",
        "                augmented_images.append(batch[0])\n",
        "                break\n",
        "    return np.array(augmented_images)\n",
        "\n",
        "# Application avant le split\n",
        "# print(\"Augmentation des données...\")\n",
        "# augmented_images = augment_images(all_images)\n",
        "# all_images = np.concatenate([all_images, augmented_images])\n",
        "# print(f\"Nouveau nombre total d'images: {len(all_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "w0JJYdzN6a3p",
        "outputId": "e9867b7f-aead-4c0d-f52c-c3d31d8601ca"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(images):\n",
        "    \"\"\"Normalisation et redimensionnement\"\"\"\n",
        "    images = images.astype('float32') / 255.  # Normalisation\n",
        "    return images\n",
        "\n",
        "# Split train/test\n",
        "x_train, x_test = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "# Application du prétraitement\n",
        "x_train = preprocess_data(x_train)\n",
        "x_test = preprocess_data(x_test)\n",
        "\n",
        "print(f\"\\nForme des données :\")\n",
        "print(f\"Train: {x_train.shape} (ex: {x_train[0].shape})\")\n",
        "print(f\"Nombre d'images d'entrainement : {len(x_train) }\")\n",
        "print(f\"Test: {x_test.shape} (ex: {x_test[0].shape})\")\n",
        "print(f\"Nombre d'images test : {len(x_test) }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FyJdTzP6a3p"
      },
      "outputs": [],
      "source": [
        "display_images(x_train, num_samples=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HvWpCfl6a3q"
      },
      "source": [
        "## Auto-encodeur de réduction de bruit (denoiser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ercmZR086a3q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_noise(images, noise_type=\"gaussian\", noise_factor=0.3):\n",
        "    noisy_images = images.copy()\n",
        "\n",
        "    # if noise_type == \"gaussian\":\n",
        "    #     noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)\n",
        "    #     noisy_images += noise\n",
        "\n",
        "    # elif noise_type == \"poisson\":\n",
        "    #     noise = np.random.poisson(images * 255.0) / 255.0  # Convert to 0-1 range\n",
        "    #     noisy_images += noise\n",
        "\n",
        "    if noise_type == \"speckle\":\n",
        "        noise = noise_factor * np.random.randn(*images.shape)\n",
        "        noisy_images += images * noise\n",
        "\n",
        "    # if noise_type == \"salt_pepper\":\n",
        "    #     prob = noise_factor / 2\n",
        "    #     rnd = np.random.rand(*images.shape)\n",
        "    #     noisy_images[rnd < prob] = 0  # Sel (noir)\n",
        "    #     noisy_images[rnd > (1 - prob)] = 1  # Poivre (blanc)\n",
        "\n",
        "    return np.clip(noisy_images, 0., 1.)\n",
        "\n",
        "# Appliquer plusieurs types de bruits\n",
        "# x_train_noisy_gaussian = add_noise(x_train, \"gaussian\")\n",
        "# x_test_noisy_gaussian = add_noise(x_test, \"gaussian\")\n",
        "\n",
        "x_train_noisy_speckle = add_noise(x_train, \"speckle\")\n",
        "x_test_noisy_speckle = add_noise(x_test, \"speckle\")\n",
        "\n",
        "# x_train_noisy_sp = add_noise(x_train, \"salt_pepper\")\n",
        "# x_test_noisy_sp = add_noise(x_test, \"salt_pepper\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSa1gFmx6a3q"
      },
      "outputs": [],
      "source": [
        "display_images(x_train, num_samples=10)\n",
        "# display_images(x_train_noisy_gaussian, num_samples=10)\n",
        "display_images(x_train_noisy_speckle, num_samples=10)\n",
        "# display_images(x_train_noisy_sp, num_samples=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO-mAuN16a3q"
      },
      "outputs": [],
      "source": [
        "print(\"Dimensions des données :\")\n",
        "print(\"x_test shape:\", x_test.shape)          # Doit être (N, 256, 256, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3aK2laH6a3r"
      },
      "outputs": [],
      "source": [
        "# Configurations principales de nos modèles\n",
        "IMG_SIZE          = img_size                # taille coté final d'une image en pixel (ici 128x128)\n",
        "NB_EPOCHS_DENOISE = 50               # nombre epoch alogithme debruiter\n",
        "BATCH_SIZE        = 32               # taille batch de traitement\n",
        "SAV_MODEL_DENOISE = \"denoiser3.h5\"     # sauvegarde du modele de debruitage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dXGFhe06a3r"
      },
      "source": [
        "## Encodeur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwzZxiTK6a3r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Input\n",
        "\n",
        "\n",
        "# Data augmentation\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip('horizontal'),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.1)\n",
        "    ],\n",
        "    name=\"data_augmentation\"\n",
        ")\n",
        "\n",
        "# Entrée de l'image\n",
        "input_img = Input(shape=(128, 128, 3))  # à adapter selon ta taille d’image\n",
        "\n",
        "# Applique la data augmentation directement à l'entrée\n",
        "x = data_augmentation(input_img)\n",
        "\n",
        "# Bloc 1\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Bloc 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Bloc 3\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrGIDQM56a3r"
      },
      "source": [
        "## Decodeur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoW4O1BK6a3r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import UpSampling2D, Dropout, Conv2D, BatchNormalization\n",
        "\n",
        "# Bloc 1\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = Dropout(0.3)(x)  # Dropout un peu plus fort ici car c’est la couche la plus profonde\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "\n",
        "# Bloc 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "\n",
        "# Bloc 3\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "\n",
        "# Dernière couche (sortie)\n",
        "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RudsBP3I6a3s"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse',  metrics=['accuracy'])  # MSE au lieu de binary_crossentropy pour les images continues\n",
        "autoencoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvDqjVfN6a3s"
      },
      "source": [
        "## Entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGNiVdKm6a3s"
      },
      "outputs": [],
      "source": [
        "#%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILBRszLp6a3s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# history = autoencoder.fit(\n",
        "    # x_train_noisy_gaussian, x_train,\n",
        "    # epochs=50,\n",
        "    # batch_size=64,\n",
        "    # shuffle=True,\n",
        "    # validation_data=(x_test_noisy_gaussian, x_test)\n",
        "# )\n",
        "\n",
        "history_speckle = autoencoder.fit(\n",
        "    x_train_noisy_speckle, x_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test_noisy_speckle, x_test)\n",
        ")\n",
        "\n",
        "# history_sp = autoencoder.fit(\n",
        "#     x_train_noisy_sp, x_train,\n",
        "#     epochs=50,\n",
        "#     batch_size=32,\n",
        "#     shuffle=True,\n",
        "#     validation_data=(x_test_noisy_sp, x_test)\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSFwRx2O6a3s"
      },
      "outputs": [],
      "source": [
        "# Visualisation des pertes d'apprentissage (Train) et de validation (Test)\n",
        "# plt.plot(history.history['loss'], label='train')  # Pertes d'apprentissage\n",
        "# plt.plot(history.history['val_loss'], label='test')  # Pertes de validation\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Courbe d\\'apprentissage')\n",
        "plt.show()\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Gaussian Noise\n",
        "# axes[0].plot(history.history['loss'], label='Train')\n",
        "# axes[0].plot(history.history['val_loss'], label='Test')\n",
        "# axes[0].set_title(\"Gaussian Noise\")\n",
        "# axes[0].set_xlabel('Epochs')\n",
        "# axes[0].set_ylabel('Loss')\n",
        "# axes[0].legend()\n",
        "\n",
        "# Speckle Noise\n",
        "axes[1].plot(history_speckle.history['loss'], label='Train')\n",
        "axes[1].plot(history_speckle.history['val_loss'], label='Test')\n",
        "axes[1].set_title(\"Speckle Noise\")\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].legend()\n",
        "\n",
        "# Salt & Pepper Noise\n",
        "# axes[2].plot(history_sp.history['loss'], label='Train')\n",
        "# axes[2].plot(history_sp.history['val_loss'], label='Test')\n",
        "# axes[2].set_title(\"Salt & Pepper Noise\")\n",
        "# axes[2].set_xlabel('Epochs')\n",
        "# axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOxqhY1t6a3s"
      },
      "outputs": [],
      "source": [
        "#autoencoder.save(SAV_MODEL_DENOISE)  # Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvXI-ynT6a3t"
      },
      "outputs": [],
      "source": [
        "print(\"Dimensions des données :\")\n",
        "print(\"x_test shape:\", x_test.shape)          # Doit être (N, 256, 256, 3)\n",
        "\n",
        "print(\"Modèle input shape:\", autoencoder.input_shape)  # Doit être (None, 256, 256, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd0uY7-b6a3t"
      },
      "outputs": [],
      "source": [
        "def visualize_denoising(model, clean_images, noisy_images, num_samples=3):\n",
        "    \"\"\"Version corrigée avec affichage garanti\"\"\"\n",
        "    # 1. Vérification des entrées\n",
        "    num_samples = min(num_samples, len(clean_images))\n",
        "    print(f\"\\nVisualisation de {num_samples} échantillons...\")\n",
        "\n",
        "    # 2. Prédiction\n",
        "    try:\n",
        "        denoised = model.predict(noisy_images[:num_samples], verbose=0)\n",
        "        print(\"Prédiction réussie. Shapes :\")\n",
        "        print(f\"- Original : {clean_images[0].shape}\")\n",
        "        print(f\"- Bruité : {noisy_images[0].shape}\")\n",
        "        print(f\"- Débruité : {denoised[0].shape}\")\n",
        "    except Exception as e:\n",
        "        print(\"\\nÉchec de prédiction :\")\n",
        "        print(f\"Erreur : {str(e)}\")\n",
        "        print(\"Vérifiez que :\")\n",
        "        print(\"- Le modèle est compilé\")\n",
        "        print(\"- noisy_images.shape =\", noisy_images.shape)\n",
        "        print(\"- model.input_shape =\", model.input_shape)\n",
        "        return\n",
        "\n",
        "    # 3. Configuration du plot\n",
        "    plt.figure(figsize=(15, 8), dpi=100)  # Augmentation de la résolution\n",
        "\n",
        "    # 4. Affichage des images\n",
        "    for i in range(num_samples):\n",
        "        # Calcul des métriques\n",
        "        try:\n",
        "            psnr_n = psnr(clean_images[i], noisy_images[i])\n",
        "            psnr_d = psnr(clean_images[i], denoised[i])\n",
        "        except:\n",
        "            psnr_n = psnr_d = float('nan')  # En cas d'échec du calcul\n",
        "\n",
        "        # Original\n",
        "        plt.subplot(3, num_samples, i+1)\n",
        "        plt.imshow(clean_images[i].squeeze(), cmap='gray' if clean_images[i].shape[-1] == 1 else None)\n",
        "        plt.title(f\"Original\\nPSNR: ∞ dB\", fontsize=8)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Bruité\n",
        "        plt.subplot(3, num_samples, i+1+num_samples)\n",
        "        plt.imshow(noisy_images[i].squeeze(), cmap='gray' if noisy_images[i].shape[-1] == 1 else None)\n",
        "        plt.title(f\"Noisy\\nPSNR: {psnr_n:.2f} dB\", fontsize=8)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Débruité\n",
        "        plt.subplot(3, num_samples, i+1+2*num_samples)\n",
        "        plt.imshow(denoised[i].squeeze(), cmap='gray' if denoised[i].shape[-1] == 1 else None)\n",
        "        plt.title(f\"Denoised\\nPSNR: {psnr_d:.2f} dB\", fontsize=8)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # 5. Affichage final\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"Visualisation terminée!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVp2tVgy6a3t"
      },
      "outputs": [],
      "source": [
        "print(\"Dimensions des données :\")\n",
        "print(\"x_test shape:\", x_test.shape)          # Doit être (N, 256, 256, 3)\n",
        "# print(\"x_test_noisy shape:\", x_test_noisy_gaussian.shape)  # Doit être (N, 256, 256, 3)\n",
        "# print(\"x_test_noisy shape:\", x_test_noisy_sp.shape)  # Doit être (N, 256, 256, 3)\n",
        "print(\"x_test_noisy shape:\", x_train_noisy_speckle.shape)  # Doit être (N, 256, 256, 3)\n",
        "\n",
        "print(\"Modèle input shape:\", autoencoder.input_shape)  # Doit être (None, 256, 256, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jizbdqAC6a3t"
      },
      "outputs": [],
      "source": [
        "# # Test minimal\n",
        "# print(\"=== Test de visualisation ===\")\n",
        "# print(\"x_test shape:\", x_test.shape)          # Doit être (N, 256, 256, 3)\n",
        "# # print(\"x_test_noisy shape:\", x_test_noisy_gaussian.shape)  # Doit être (N, 256, 256, 3)\n",
        "# # print(\"x_test_noisy shape:\", x_test_noisy_sp.shape)  # Doit être (N, 256, 256, 3)\n",
        "visualize_denoising(autoencoder, x_test[:3], x_train_noisy_speckle[:3])  # Test avec 3 échantillons\n",
        "# visualize_denoising(autoencoder, x_test[:3], x_train_noisy_sp[:4])  # Test avec 3 échantillons"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
