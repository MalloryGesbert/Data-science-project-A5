{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - classification d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des dépendances\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dataset\n",
    "dataset_path = \"./Dataset/Dataset1\" # Chemin vers le dataset à modifier\n",
    "\n",
    "# Vérification de l'existence du dossier\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Le dossier {dataset_path} n'existe pas.\")\n",
    "\n",
    "# Récupération des classes et du nombre d'images par classe\n",
    "classes = []\n",
    "image_counts = []\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Parcours des fichiers dans le dataset\n",
    "for class_name in os.listdir(dataset_path):  # Parcours des fichiers dans le dataset\n",
    "    class_path = os.path.join(dataset_path, class_name)  # Chemin vers le fichier\n",
    "    classes.append(class_name.replace('Dataset Livrable 1 - ',''))  # Ajout du nom de la classe sans l'extension\n",
    "    image_counts.append(len(os.listdir(class_path)))  # Comptage des fichiers dans le dossier\n",
    "\n",
    "# Affichage des classes et du nombre d'images par classe\n",
    "print(\"Classes : \", classes)\n",
    "print(\"Nombre d'images par classe : \", image_counts)\n",
    "\n",
    "# Affichage de l'histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(classes, image_counts, color='skyblue')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Nombre d\\'images')\n",
    "plt.title('Répartition du nombre de données (images) par classe')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def display_random_images(dataset_path):\n",
    "    \"\"\"\n",
    "    Affiche un nombre aléatoire d'images d'une classe donnée.\n",
    "    \n",
    "    :param dataset_path: Chemin vers le dataset\n",
    "    :param num_images: Nombre d'images à afficher (par défaut 5)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(classes), figsize=(15, 5))\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = os.listdir(class_path)\n",
    "        random_image = random.choice(images)\n",
    "        img_path = os.path.join(class_path, random_image)\n",
    "        img = PIL.Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(class_name)\n",
    "        axes[idx].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_random_images(dataset_path)  # Affichage de 5 images aléatoires d'une classe donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppresion des fichiers corrompus ou non images --------------------------------------------------------------------\n",
    "def clean_images_dataset(dataset_path_arg):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer le dataset en supprimant les fichiers corrompus ou non images.\n",
    "    \"\"\"\n",
    "    # Dictionnaire pour stocker le nombre d'images corrompues par classe\n",
    "    corrupted_count_by_class = defaultdict(int)\n",
    "    dataset_path = dataset_path_arg\n",
    "    print(\"Début de la vérification des images ...\")\n",
    "\n",
    "    # Récupération de toutes les images pour calculer la progression\n",
    "    all_files = []\n",
    "    for dir_name in os.listdir(dataset_path): \n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            all_files.append((dir_name, dir_path, file_name))\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    checked_files = 0  # Pour la progression\n",
    "\n",
    "    # Parcours des images avec affichage de la progression\n",
    "    for dir_name, dir_path, file_name in all_files:\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            try:\n",
    "                with open(os.path.join(dir_path, file_name), 'rb') as file:\n",
    "                    img_bytes = file.read()  # Lire les bytes de l'image\n",
    "                    img = tf.image.decode_image(img_bytes)  # Essayer de décoder l'image\n",
    "            except Exception as e:\n",
    "                corrupted_count_by_class[dir_name] += 1\n",
    "                print(f\"\\nImage corrompue : {file_name} dans {dir_name}. Exception: {e}\")\n",
    "                os.remove(os.path.join(dir_path, file_name))\n",
    "                print(f\"Image {file_name} supprimée.\")\n",
    "        else:\n",
    "            corrupted_count_by_class[dir_name] += 1\n",
    "            print(f\"\\nLe fichier {file_name} dans {dir_name} n'est pas une image.\")\n",
    "            os.remove(os.path.join(dir_path, file_name))\n",
    "            print(f\"Fichier {file_name} supprimé.\")\n",
    "\n",
    "        # Mise à jour de la progression\n",
    "        checked_files += 1\n",
    "        progress = (checked_files / total_files) * 100\n",
    "        print(f\"\\rProgression : [{int(progress)}%] {checked_files}/{total_files} images vérifiées\", end=\"\")\n",
    "\n",
    "    print(\"\\nVérification des fichiers terminée.\")\n",
    "\n",
    "    # Affichage du nombre d'images corrompues par dossier\n",
    "    for dir_name, count in corrupted_count_by_class.items():\n",
    "        print(f\"Dossier {dir_name} : {count} images corrompues\")\n",
    "\n",
    "    # Nombre total d'images corrompues\n",
    "    total_corrupted = sum(corrupted_count_by_class.values())\n",
    "    print(f\"Nombre total d'images corrompues ou non image : {total_corrupted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def separate_images(source_folder):\n",
    "    # Définition des dossiers de destination\n",
    "    jpg_folder = source_folder + \"_jpg\"\n",
    "    png_folder = source_folder + \"_png\"\n",
    "    \n",
    "    # Création des dossiers s'ils n'existent pas\n",
    "    os.makedirs(jpg_folder, exist_ok=True)\n",
    "    os.makedirs(png_folder, exist_ok=True)\n",
    "    \n",
    "    # Parcours des fichiers dans le dossier source\n",
    "    for filename in os.listdir(source_folder):\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # Vérification que c'est bien un fichier\n",
    "        if os.path.isfile(file_path):\n",
    "            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpeg\"):\n",
    "                shutil.move(file_path, os.path.join(jpg_folder, filename))\n",
    "            elif filename.lower().endswith(\".png\"):\n",
    "                shutil.move(file_path, os.path.join(png_folder, filename))\n",
    "    \n",
    "    print(\"Séparation terminée !\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "source_directory = \"./Dataset/Dataset1/Sketch/\"\n",
    "separate_images(source_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (180, 180)  # Taille des images\n",
    "batch_size = 32  # Taille du lot\n",
    "\n",
    "\n",
    "# chargement des images\n",
    "# Le train_set\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  dataset_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=42,\n",
    "  batch_size=batch_size,\n",
    "  image_size=image_size\n",
    ")\n",
    "\n",
    "# Le test_set\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  dataset_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=42,\n",
    "  batch_size=batch_size,\n",
    "  image_size=image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_set.class_names\n",
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générateur de données avec augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),  # Flip horizontal et vertical\n",
    "    layers.RandomRotation(0.2),  # Rotation aléatoire\n",
    "    layers.RandomZoom(0.2),  # Zoom aléatoire\n",
    "])\n",
    "\n",
    "# Exemple d'application de la data augmentation sur un lot d'images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_set.take(1):  # Prendre un lot d'images du train_set\n",
    "    for i in range(9):  # Afficher 9 images augmentées\n",
    "        augmented_image = data_augmentation(images[i:i+1])  # Appliquer la data augmentation\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Dossier contenant les images de la classe sous-représentée\n",
    "minority_class_path = \"Dataset/Dataset1/Sketch\"\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, \n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Générer de nouvelles images\n",
    "images = os.listdir(minority_class_path)\n",
    "for image_name in images:\n",
    "    img_path = os.path.join(minority_class_path, image_name)\n",
    "    img = load_img(img_path)  # Charger l'image\n",
    "    img = img_to_array(img)   # Convertir en tableau numpy\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(img, batch_size=1, save_to_dir=minority_class_path, save_prefix='aug', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i >= 2:  # Générer environ 2 nouvelles images par image existante (pour arriver à 10k)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ponderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Définition des classes et des étiquettes\n",
    "y_train = np.array(classes)  # Remplace par tes labels\n",
    "\n",
    "# Calcul des poids\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Poids des classes :\", class_weights_dict)\n",
    "# Entraînement du modèle avec pondération\n",
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, class_weight=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Categorical = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", input_shape=(image_size, 3)), \n",
    "    layers.RandomRotation(0.2), \n",
    "    layers.RandomZoom(0.2), \n",
    "    layers.Rescaling(1./255, input_shape=(image_size, 3)),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "model_Categorical.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "model_Categorical.summary()\n",
    "\n",
    "history = model_Categorical.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Binary = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", input_shape=(image_size, 3)), \n",
    "    layers.RandomRotation(0.2), \n",
    "    layers.RandomZoom(0.2), \n",
    "    layers.Rescaling(1./255, input_shape=(image_size, 3)),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model_Binary.compile(\n",
    "    optimizer='adam', \n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_Binary.summary()\n",
    "\n",
    "history = model_Binary.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "precision = history.history.get('precision', [])  \n",
    "val_precision = history.history.get('val_precision', [])\n",
    "\n",
    "recall = history.history.get('recall', [])  \n",
    "val_recall = history.history.get('val_recall', [])\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Graphique 1 - Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Graphique 2 - Loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Graphique 3 - Precision (si disponible)\n",
    "if precision and val_precision:\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_range, precision, label='Training Precision')\n",
    "    plt.plot(epochs_range, val_precision, label='Validation Precision')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Precision')\n",
    "\n",
    "# Graphique 4 - Recall (si disponible)\n",
    "if recall and val_recall:\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_range, recall, label='Training Recall')\n",
    "    plt.plot(epochs_range, val_recall, label='Validation Recall')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Multifilter = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", input_shape=(image_size, 3)), \n",
    "    layers.RandomRotation(0.2), \n",
    "    layers.RandomZoom(0.2), \n",
    "    layers.Rescaling(1./255, input_shape=(image_size, 3)),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "model_Multifilter.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "model_Multifilter.summary()\n",
    "\n",
    "history = model_Multifilter.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "precision = history.history.get('precision', [])  \n",
    "val_precision = history.history.get('val_precision', [])\n",
    "\n",
    "recall = history.history.get('recall', [])  \n",
    "val_recall = history.history.get('val_recall', [])\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Graphique 1 - Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Graphique 2 - Loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Graphique 3 - Precision (si disponible)\n",
    "if precision and val_precision:\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_range, precision, label='Training Precision')\n",
    "    plt.plot(epochs_range, val_precision, label='Validation Precision')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Precision')\n",
    "\n",
    "# Graphique 4 - Recall (si disponible)\n",
    "if recall and val_recall:\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_range, recall, label='Training Recall')\n",
    "    plt.plot(epochs_range, val_recall, label='Validation Recall')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
