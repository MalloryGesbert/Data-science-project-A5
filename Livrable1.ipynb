{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - classification d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dépendances\n",
    "Les dépandances nécessaires à la bonne exécution du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des dépendances\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du dataset\n",
    "Dans cette partie, nous allons analyser le dataset afin d'identifier les actions nécessaires à son utilisation pour entraîner notre IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations sur le dataset\n",
    "Nous parcourons le dataset pour récupérer les différentes classes ainsi que le nombre de données (images) par classe.\n",
    "\n",
    "Nous affichons ensuite un histogramme pour une meilleure visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dataset\n",
    "dataset_path = \"./Dataset/Dataset1\" # Chemin vers le dataset à modifier\n",
    "\n",
    "# Vérification de l'existence du dossier\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Le dossier {dataset_path} n'existe pas.\")\n",
    "\n",
    "# Récupération des classes et du nombre d'images par classe\n",
    "classes = []\n",
    "image_counts = []\n",
    "\n",
    "# Parcours des fichiers dans le dataset\n",
    "for class_name in os.listdir(dataset_path):  # Parcours des fichiers dans le dataset\n",
    "    class_path = os.path.join(dataset_path, class_name)  # Chemin vers le fichier\n",
    "    classes.append(class_name.replace('Dataset Livrable 1 - ',''))  # Ajout du nom de la classe sans l'extension\n",
    "    image_counts.append(len(os.listdir(class_path)))  # Comptage des fichiers dans le dossier\n",
    "\n",
    "# Affichage des classes et du nombre d'images par classe\n",
    "print(\"Classes : \", classes)\n",
    "print(\"Nombre d'images par classe : \", image_counts)\n",
    "\n",
    "# Affichage de l'histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(classes, image_counts, color='skyblue')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Nombre d\\'images')\n",
    "plt.title('Répartition du nombre de données (images) par classe')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afficher des images\n",
    "Fonction pour afficher une image par classe de façon aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def display_random_images(dataset_path):\n",
    "    \"\"\"\n",
    "    Affiche un nombre aléatoire d'images d'une classe donnée.\n",
    "    \n",
    "    :param dataset_path: Chemin vers le dataset\n",
    "    :param num_images: Nombre d'images à afficher (par défaut 5)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(classes), figsize=(15, 5))\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = os.listdir(class_path)\n",
    "        random_image = random.choice(images)\n",
    "        img_path = os.path.join(class_path, random_image)\n",
    "        img = PIL.Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(class_name)\n",
    "        axes[idx].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display_random_images(dataset_path)  # Affichage de 5 images aléatoires d'une classe donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données\n",
    "La fonction suivante permet de parcourir le dataset et de supprimer les fichiers non images et les images corrompus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppresion des fichiers corrompus ou non images --------------------------------------------------------------------\n",
    "def clean_images_dataset(dataset_path_arg):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer le dataset en supprimant les fichiers corrompus ou non images.\n",
    "    \"\"\"\n",
    "    # Dictionnaire pour stocker le nombre d'images corrompues par classe\n",
    "    corrupted_count_by_class = defaultdict(int)\n",
    "    dataset_path = dataset_path_arg\n",
    "    print(\"Début de la vérification des images ...\")\n",
    "\n",
    "    # Récupération de toutes les images pour calculer la progression\n",
    "    all_files = []\n",
    "    for dir_name in os.listdir(dataset_path): \n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            all_files.append((dir_name, dir_path, file_name))\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    checked_files = 0  # Pour la progression\n",
    "\n",
    "    # Parcours des images avec affichage de la progression\n",
    "    for dir_name, dir_path, file_name in all_files:\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            try:\n",
    "                with open(os.path.join(dir_path, file_name), 'rb') as file:\n",
    "                    img_bytes = file.read()  # Lire les bytes de l'image\n",
    "                    img = tf.image.decode_image(img_bytes)  # Essayer de décoder l'image\n",
    "            except Exception as e:\n",
    "                corrupted_count_by_class[dir_name] += 1\n",
    "                print(f\"\\nImage corrompue : {file_name} dans {dir_name}. Exception: {e}\")\n",
    "                os.remove(os.path.join(dir_path, file_name))\n",
    "                print(f\"Image {file_name} supprimée.\")\n",
    "        else:\n",
    "            corrupted_count_by_class[dir_name] += 1\n",
    "            print(f\"\\nLe fichier {file_name} dans {dir_name} n'est pas une image.\")\n",
    "            os.remove(os.path.join(dir_path, file_name))\n",
    "            print(f\"Fichier {file_name} supprimé.\")\n",
    "\n",
    "        # Mise à jour de la progression\n",
    "        checked_files += 1\n",
    "        progress = (checked_files / total_files) * 100\n",
    "        print(f\"\\rProgression : [{int(progress)}%] {checked_files}/{total_files} images vérifiées\", end=\"\")\n",
    "\n",
    "    print(\"\\nVérification des fichiers terminée.\")\n",
    "\n",
    "    # Affichage du nombre d'images corrompues par dossier\n",
    "    for dir_name, count in corrupted_count_by_class.items():\n",
    "        print(f\"Dossier {dir_name} : {count} images corrompues\")\n",
    "\n",
    "    # Nombre total d'images corrompues\n",
    "    total_corrupted = sum(corrupted_count_by_class.values())\n",
    "    print(f\"Nombre total d'images corrompues ou non image : {total_corrupted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_images_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour séparer les images jpg et png d'un dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_images(source_folder):\n",
    "    # Définition des dossiers de destination\n",
    "    jpg_folder = source_folder + \"_jpg\"\n",
    "    png_folder = source_folder + \"_png\"\n",
    "    \n",
    "    # Création des dossiers s'ils n'existent pas\n",
    "    os.makedirs(jpg_folder, exist_ok=True)\n",
    "    os.makedirs(png_folder, exist_ok=True)\n",
    "    \n",
    "    # Parcours des fichiers dans le dossier source\n",
    "    for filename in os.listdir(source_folder):\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # Vérification que c'est bien un fichier\n",
    "        if os.path.isfile(file_path):\n",
    "            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpeg\"):\n",
    "                shutil.move(file_path, os.path.join(jpg_folder, filename))\n",
    "            elif filename.lower().endswith(\".png\"):\n",
    "                shutil.move(file_path, os.path.join(png_folder, filename))\n",
    "    \n",
    "    print(\"Séparation terminée !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate_images(\"./Dataset/Dataset1/Sketch/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données\n",
    "Nous déclarons également nos variables pour le futur entraînement de notre modèle.\n",
    "\n",
    "Nous importons nos données grâce à TensorFlow afin de les manipuler par la suite avec cette librairie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (180, 180)  # Taille des images\n",
    "batch_size = 32  # Taille du lot\n",
    "epochs = 100  # Nombre d'époques\n",
    "\n",
    "# chargement des images\n",
    "# Le train_set\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  dataset_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=42,\n",
    "  batch_size=batch_size,\n",
    "  image_size=image_size\n",
    ")\n",
    "\n",
    "# Le test_set\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  dataset_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=42,\n",
    "  batch_size=batch_size,\n",
    "  image_size=image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration du dataset\n",
    "Grâce à notre exploration des données précédentes, nous avons remarqué que l'une de nos classes (Sketch) possède un nombre de données bien inférieur aux autres classes.\n",
    "\n",
    "Pour y remédier, plusieurs solutions sont envisageables :\n",
    "- Augmenter la taille du dataset en ajoutant de nouvelles données provenant d'autres datasets.\n",
    "- Générer de nouvelles données en appliquant des transformations sur les données existantes (rotations, zooms, flou, etc.).\n",
    "- Ajuster la pondération des classes afin de compenser ce déséquilibre et améliorer l'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de nouvelles données au dataset\n",
    "Augmente la taille du dataset en ajoutant de nouvelles données provenant d'autres datasets.\n",
    "- Dataset pour les sketch jpg : https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/\n",
    "- Dataset pour les sketch png : https://www.kaggle.com/datasets/arbazkhan971/cuhk-face-sketch-database-cufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download_kaggle_dataset(dataset_name, destination_folder):\n",
    "    \"\"\"\n",
    "    Télécharge un dataset Kaggle en utilisant l'API Kaggle.\n",
    "    Assurez-vous d'avoir configuré votre fichier kaggle.json dans ~/.kaggle/.\n",
    "    \"\"\"\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    os.system(f\"kaggle datasets download -d {dataset_name} -p {destination_folder}\")\n",
    "    zip_path = os.path.join(destination_folder, f\"{dataset_name.split('/')[-1]}.zip\")\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Dataset {dataset_name} téléchargé et extrait dans {destination_folder}.\")\n",
    "\n",
    "def download_http_dataset(url, destination_folder):\n",
    "    \"\"\"\n",
    "    Télécharge un dataset depuis une URL HTTP.\n",
    "    \"\"\"\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    file_name = os.path.join(destination_folder, url.split('/')[-1])\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"Fichier téléchargé : {file_name}\")\n",
    "        if file_name.endswith('.zip'):\n",
    "            with ZipFile(file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(destination_folder)\n",
    "            os.remove(file_name)\n",
    "            print(f\"Fichier extrait dans {destination_folder}.\")\n",
    "    else:\n",
    "        print(f\"Échec du téléchargement depuis {url}.\")\n",
    "def add_datas_to_dataset():\n",
    "    \"\"\"\n",
    "    Fonction pour ajouter des données au dataset.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger le dataset Kaggle\n",
    "download_kaggle_dataset(\"arbazkhan971/cuhk-face-sketch-database-cufs\", \"./Dataset/CUHK Face Sketch Database\")\n",
    "\n",
    "# Télécharger le dataset depuis une URL HTTP\n",
    "#download_http_dataset(\"https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/\", \"./datasets/classifysketch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datas augmentation\n",
    "Génére de nouvelles données en appliquant des transformations sur les données existantes (rotations, zooms, flou, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datas_augmentation(dataset_path, num_images_gen=2):\n",
    "    \"\"\"\n",
    "    Fonction pour générer des images augmentées à partir d'un dataset existant pour toutes les classes.\n",
    "    \"\"\"\n",
    "    # Data Augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Générer de nouvelles images\n",
    "    dir_class = os.listdir(dataset_path)\n",
    "    for dir in dir_class:\n",
    "        dir_path = os.path.join(dataset_path, dir)\n",
    "        images = os.listdir(dir_path)\n",
    "        for image_name in images:\n",
    "            img_path = os.path.join(dataset_path, image_name)\n",
    "            img = load_img(img_path)  # Charger l'image\n",
    "            img = img_to_array(img)   # Convertir en tableau numpy\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "\n",
    "            i = 0\n",
    "            for batch in datagen.flow(img, batch_size=1, save_to_dir=dataset_path, save_prefix='aug', save_format='jpeg'):\n",
    "                i += 1\n",
    "                if i >= num_images_gen:  # Générer environ X nouvelles images par image existante (pour arriver à 10k)\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datas_augmentation(dataset_path, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ponderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def compute_class_weights(classes):\n",
    "    \"\"\"\n",
    "    Fonction pour calculer les poids des classes.\n",
    "    \"\"\"\n",
    "    # Définition des classes et des étiquettes\n",
    "    y_train = np.array(classes)  # Remplace par tes labels\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    return class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"Poids des classes :\", compute_class_weights(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
