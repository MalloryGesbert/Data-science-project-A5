{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - classification d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dépendances\n",
    "Les dépandances nécessaires à la bonne exécution du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des dépendances\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import random\n",
    "import kaggle\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du dataset\n",
    "Dans cette partie, nous allons analyser le dataset afin d'identifier les actions nécessaires à son utilisation pour entraîner notre IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations sur le dataset\n",
    "Nous parcourons le dataset pour récupérer les différentes classes ainsi que le nombre de données (images) par classe.\n",
    "\n",
    "Nous affichons ensuite un histogramme pour une meilleure visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dataset\n",
    "dataset_path = \"./Dataset/Dataset1\" # Chemin vers le dataset à modifier\n",
    "\n",
    "# Vérification de l'existence du dossier\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Le dossier {dataset_path} n'existe pas.\")\n",
    "\n",
    "# Récupération des classes et du nombre d'images par classe\n",
    "classes = []\n",
    "image_counts = []\n",
    "\n",
    "# Parcours des fichiers dans le dataset\n",
    "for class_name in os.listdir(dataset_path):  # Parcours des fichiers dans le dataset\n",
    "    class_path = os.path.join(dataset_path, class_name)  # Chemin vers le fichier\n",
    "    classes.append(class_name.replace('Dataset Livrable 1 - ',''))  # Ajout du nom de la classe sans l'extension\n",
    "    image_counts.append(len(os.listdir(class_path)))  # Comptage des fichiers dans le dossier\n",
    "\n",
    "# Affichage des classes et du nombre d'images par classe\n",
    "print(\"Classes : \", classes)\n",
    "print(\"Nombre d'images par classe : \", image_counts)\n",
    "\n",
    "# Affichage de l'histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(classes, image_counts, color='skyblue')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Nombre d\\'images')\n",
    "plt.title('Répartition du nombre de données (images) par classe')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afficher des images\n",
    "Fonction pour afficher une image aléatoire par classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def display_random_images(dataset_path):\n",
    "    \"\"\"\n",
    "    Affiche un nombre aléatoire d'images d'une classe donnée.\n",
    "    \n",
    "    :param dataset_path: Chemin vers le dataset\n",
    "    :param num_images: Nombre d'images à afficher (par défaut 5)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(classes), figsize=(15, 5))\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = os.listdir(class_path)\n",
    "        random_image = random.choice(images)\n",
    "        img_path = os.path.join(class_path, random_image)\n",
    "        img = PIL.Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(class_name)\n",
    "        axes[idx].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display_random_images(dataset_path)  # Affichage de 5 images aléatoires d'une classe donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données\n",
    "La fonction suivante permet de parcourir le dataset et de supprimer les fichiers non images et les images corrompus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppresion des fichiers corrompus ou non images --------------------------------------------------------------------\n",
    "def clean_images_dataset(dataset_path_arg):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer le dataset en supprimant les fichiers corrompus ou non images.\n",
    "    \"\"\"\n",
    "    # Dictionnaire pour stocker le nombre d'images corrompues par classe\n",
    "    corrupted_count_by_class = defaultdict(int)\n",
    "    dataset_path = dataset_path_arg\n",
    "    print(\"Début de la vérification des images ...\")\n",
    "\n",
    "    # Récupération de toutes les images pour calculer la progression\n",
    "    all_files = []\n",
    "    for dir_name in os.listdir(dataset_path): \n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            all_files.append((dir_name, dir_path, file_name))\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    checked_files = 0  # Pour la progression\n",
    "\n",
    "    # Parcours des images avec affichage de la progression\n",
    "    for dir_name, dir_path, file_name in all_files:\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            try:\n",
    "                with open(os.path.join(dir_path, file_name), 'rb') as file:\n",
    "                    img_bytes = file.read()  # Lire les bytes de l'image\n",
    "                    img = tf.image.decode_image(img_bytes)  # Essayer de décoder l'image\n",
    "            except Exception as e:\n",
    "                corrupted_count_by_class[dir_name] += 1\n",
    "                print(f\"\\nImage corrompue : {file_name} dans {dir_name}. Exception: {e}\")\n",
    "                os.remove(os.path.join(dir_path, file_name))\n",
    "                print(f\"Image {file_name} supprimée.\")\n",
    "        else:\n",
    "            corrupted_count_by_class[dir_name] += 1\n",
    "            print(f\"\\nLe fichier {file_name} dans {dir_name} n'est pas une image.\")\n",
    "            os.remove(os.path.join(dir_path, file_name))\n",
    "            print(f\"Fichier {file_name} supprimé.\")\n",
    "\n",
    "        # Mise à jour de la progression\n",
    "        checked_files += 1\n",
    "        progress = (checked_files / total_files) * 100\n",
    "        print(f\"\\rProgression : [{int(progress)}%] {checked_files}/{total_files} images vérifiées\", end=\"\")\n",
    "\n",
    "    print(\"\\nVérification des fichiers terminée.\")\n",
    "\n",
    "    # Affichage du nombre d'images corrompues par dossier\n",
    "    for dir_name, count in corrupted_count_by_class.items():\n",
    "        print(f\"Dossier {dir_name} : {count} images corrompues\")\n",
    "\n",
    "    # Nombre total d'images corrompues\n",
    "    total_corrupted = sum(corrupted_count_by_class.values())\n",
    "    print(f\"Nombre total d'images corrompues ou non image : {total_corrupted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_images_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour séparer les images jpg et png d'un dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_images(source_folder):\n",
    "    # Définition des dossiers de destination\n",
    "    jpg_folder = source_folder + \"_jpg\"\n",
    "    png_folder = source_folder + \"_png\"\n",
    "    \n",
    "    # Création des dossiers s'ils n'existent pas\n",
    "    os.makedirs(jpg_folder, exist_ok=True)\n",
    "    os.makedirs(png_folder, exist_ok=True)\n",
    "    \n",
    "    # Parcours des fichiers dans le dossier source\n",
    "    for filename in os.listdir(source_folder):\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # Vérification que c'est bien un fichier\n",
    "        if os.path.isfile(file_path):\n",
    "            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpeg\"):\n",
    "                shutil.move(file_path, os.path.join(jpg_folder, filename))\n",
    "            elif filename.lower().endswith(\".png\"):\n",
    "                shutil.move(file_path, os.path.join(png_folder, filename))\n",
    "    \n",
    "    print(\"Séparation terminée !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate_images(\"./Dataset/Dataset1/Sketch/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données\n",
    "Nous déclarons également nos variables pour le futur entraînement de notre modèle.\n",
    "\n",
    "Nous importons nos données grâce à TensorFlow afin de les manipuler par la suite avec cette librairie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (180, 180)  # Taille des images\n",
    "batch_size = 32  # Taille du lot\n",
    "epochs = 100  # Nombre d'époques\n",
    "\n",
    "# chargement des images\n",
    "# Le train_set\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  dataset_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=42,\n",
    "  batch_size=batch_size,\n",
    "  image_size=image_size\n",
    ")\n",
    "\n",
    "# Le test_set\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  dataset_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=42,\n",
    "  batch_size=batch_size,\n",
    "  image_size=image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration du dataset\n",
    "Grâce à notre exploration des données précédentes, nous avons remarqué que l'une de nos classes (Sketch) possède un nombre de données bien inférieur aux autres classes.\n",
    "\n",
    "Pour y remédier, plusieurs solutions sont envisageables :\n",
    "- Augmenter la taille du dataset en ajoutant de nouvelles données provenant d'autres datasets.\n",
    "- Générer de nouvelles données en appliquant des transformations sur les données existantes (rotations, zooms, flou, etc.).\n",
    "- Ajuster la pondération des classes afin de compenser ce déséquilibre et améliorer l'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de nouvelles données au dataset\n",
    "Augmente la taille du dataset en ajoutant de nouvelles données provenant d'autres datasets.\n",
    "- Dataset pour les sketch jpg : https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/\n",
    "- Dataset pour les sketch png : https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/sketches_png.zip (https://www.kaggle.com/datasets/arbazkhan971/cuhk-face-sketch-database-cufs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(dataset_name, destination_folder):\n",
    "    \"\"\"\n",
    "    Télécharge un dataset Kaggle en utilisant l'API Kaggle.\n",
    "    Assurez-vous d'avoir configuré votre fichier kaggle.json dans ~/.kaggle/.\n",
    "    \"\"\"\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    try:\n",
    "        os.system(f\"kaggle datasets download -d {dataset_name} -p {destination_folder}\")\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Le module 'kaggle' n'est pas installé. Veuillez l'installer avec 'pip install kaggle'.\")\n",
    "    zip_path = os.path.join(destination_folder, f\"{dataset_name.split('/')[-1]}.zip\")\n",
    "    print(zip_path)\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Dataset {dataset_name} téléchargé et extrait dans {destination_folder}.\")\n",
    "\n",
    "def download_http_dataset(url, destination_folder):\n",
    "    \"\"\"\n",
    "    Télécharge un dataset depuis une URL HTTP.\n",
    "    \"\"\"\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    file_name = os.path.join(destination_folder, url.split('/')[-1])\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"Fichier téléchargé : {file_name}\")\n",
    "        if file_name.endswith('.zip'):\n",
    "            with ZipFile(file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(destination_folder)\n",
    "            os.remove(file_name)\n",
    "            print(f\"Fichier extrait dans {destination_folder}.\")\n",
    "    else:\n",
    "        print(f\"Échec du téléchargement depuis {url}.\")\n",
    "\n",
    "def delete_duplicate_images(dataset_path):\n",
    "    \"\"\"\n",
    "    Supprime les images dupliquées dans le dataset.\n",
    "    \"\"\"\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            seen_images = set()\n",
    "            for filename in os.listdir(class_path):\n",
    "                file_path = os.path.join(class_path, filename)\n",
    "                if filename in seen_images:\n",
    "                    print(f\"Image dupliquée supprimée : {file_path}\")\n",
    "                    os.remove(file_path)\n",
    "                else:\n",
    "                    seen_images.add(filename)\n",
    "\n",
    "\n",
    "def add_datas_to_dataset(dataset_path, dataset_faces_path, dataset_sketch_path):\n",
    "    \"\"\"\n",
    "    Remplace les images du dossier Sketch dans dataset_path\n",
    "    par celles venant de png/airplane et original_sketch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Chemin vers la classe cible dans le dataset principal\n",
    "    target_class_path = os.path.join(dataset_path, \"Sketch\")\n",
    "    \n",
    "    # Définir les chemins source\n",
    "    sketch_airplane_path = os.path.join(dataset_sketch_path, \"png\", \"airplane\")\n",
    "    original_sketch_path = os.path.join(dataset_faces_path, \"original_sketch\")\n",
    "\n",
    "    # Fusionner les deux dossiers source\n",
    "    for src_path in [sketch_airplane_path, original_sketch_path]:\n",
    "        if os.path.exists(src_path):\n",
    "            for image_name in os.listdir(src_path):\n",
    "                source_image_path = os.path.join(src_path, image_name)\n",
    "                if os.path.isfile(source_image_path):\n",
    "                    shutil.copy(source_image_path, target_class_path)\n",
    "\n",
    "    # Supprimer les dossiers temporaires\n",
    "    shutil.rmtree(dataset_faces_path, ignore_errors=True)\n",
    "    shutil.rmtree(dataset_sketch_path, ignore_errors=True)\n",
    "    print(f\"Dossiers temporaires supprimés :\\n- {sketch_airplane_path}\\n- {original_sketch_path}\")\n",
    "\n",
    "    print(f\"Images copiées depuis :\\n- {sketch_airplane_path}\\n- {original_sketch_path}\\nvers : {target_class_path}\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharge les datasets depuis Kaggle et HTTP, les ajoute au dataset principal et supprime les images dupliquées\n",
    "# Assurez-vous d'avoir configuré votre fichier kaggle.json dans ~/user/name/.kaggle/ pour utiliser l'API Kaggle\n",
    "\n",
    "# Télécharger le dataset Kaggle\n",
    "download_kaggle_dataset(\"arbazkhan971/cuhk-face-sketch-database-cufs\", \"./Dataset/CUHK_Face_Sketch_Database\")\n",
    "\n",
    "# Télécharger le dataset depuis une URL HTTP\n",
    "download_http_dataset(\"https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/sketches_png.zip\", \"./Dataset/classifysketch\")\n",
    "\n",
    "# Merger les datasets\n",
    "add_datas_to_dataset(dataset_path, \"./Dataset/CUHK_Face_Sketch_Database\", \"./Dataset/classifysketch\")\n",
    "\n",
    "# Supprimer les images dupliquées\n",
    "delete_duplicate_images(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datas augmentation\n",
    "Génére de nouvelles données en appliquant des transformations sur les données existantes (rotations, zooms, flou, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datas_augmentation(dataset_path, class_to_augment ,num_images_gen=2):\n",
    "    \"\"\"\n",
    "    Fonction pour générer des images augmentées à partir d'un dataset existant pour toutes les classes.\n",
    "    \"\"\"\n",
    "    # Data Augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    " \n",
    "    dir_path = os.path.join(dataset_path, class_to_augment)\n",
    "    # print(f\"Traitement du dossier : {dir_path}\")\n",
    "    for image_name in os.listdir(dir_path):\n",
    "        img_path = os.path.join(dir_path, image_name)\n",
    "        # print(f\"Traitement de l'image : {img_path}\")\n",
    "        img = load_img(img_path)  # Charger l'image\n",
    "        img = img_to_array(img)   # Convertir en tableau numpy\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img, batch_size=1, save_to_dir=dir_path, save_prefix='aug', save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i >= num_images_gen:  # Générer environ X nouvelles images par image existante (pour arriver à 10k)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datas_augmentation(dataset_path, \"Sketch\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ponderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def compute_class_weights(dataset_path):\n",
    "    \"\"\"\n",
    "    Calcule les poids des classes en fonction du nombre d'images par dossier de classe.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Chemin vers le dossier contenant les sous-dossiers de classes.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire {nom_de_classe: poids}.\n",
    "    \"\"\"\n",
    "    y_labels = []\n",
    "\n",
    "    # Parcours de chaque dossier de classe\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "\n",
    "        # Vérifie que c’est bien un dossier\n",
    "        if os.path.isdir(class_dir):\n",
    "            image_files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "            \n",
    "            # Ajoute autant de fois la classe que d'images dans le dossier\n",
    "            y_labels.extend([class_name] * len(image_files))\n",
    "\n",
    "    # Transforme la liste en tableau numpy\n",
    "    y_labels = np.array(y_labels)\n",
    "    \n",
    "    # Calcule les poids\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
    "    \n",
    "    # Crée un dictionnaire {nom_classe: poids}\n",
    "    class_weights_dict = {\n",
    "        cls: weight for cls, weight in zip(np.unique(y_labels), class_weights)\n",
    "    }\n",
    "\n",
    "    return class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Poids des classes :\", compute_class_weights(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
